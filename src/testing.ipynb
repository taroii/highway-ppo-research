{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6da254",
   "metadata": {},
   "source": [
    "## Zooming for Self Driving? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13790054",
   "metadata": {},
   "source": [
    "#### I. Class/Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2f79c4",
   "metadata": {},
   "source": [
    "##### Dependency Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "154080a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.buffers import RolloutBuffer\n",
    "from stable_baselines3.common.utils import obs_as_tensor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.distributions import CategoricalDistribution\n",
    "from stable_baselines3.common.type_aliases import Schedule\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Any, ClassVar, Type, TypeVar, Union\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "\n",
    "import highway_env # original HighwayEnv repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cbe873",
   "metadata": {},
   "source": [
    "##### Masked PPO Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e9255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked PPO Policy\n",
    "class MaskedCategoricalDistribution(CategoricalDistribution):\n",
    "    \"\"\"\n",
    "    Categorical distribution with action masking support.\n",
    "\n",
    "    Invalid actions (masked) are given probability 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_dim: int):\n",
    "        super().__init__(action_dim)\n",
    "        self.action_mask: Optional[th.Tensor] = None\n",
    "\n",
    "    def proba_distribution_net(self, latent_dim: int) -> nn.Module:\n",
    "        \"\"\"Create the action distribution network.\"\"\"\n",
    "        return nn.Linear(latent_dim, self.action_dim)\n",
    "\n",
    "    def proba_distribution(\n",
    "        self,\n",
    "        action_logits: th.Tensor,\n",
    "        action_mask: Optional[th.Tensor] = None,\n",
    "    ) -> \"MaskedCategoricalDistribution\":\n",
    "        \"\"\"\n",
    "        Set the action logits and apply masking.\n",
    "\n",
    "        Args:\n",
    "            action_logits: Logits for each action (batch_size, n_actions)\n",
    "            action_mask: Boolean mask where True = invalid (batch_size, n_actions)\n",
    "        \"\"\"\n",
    "        self.action_mask = action_mask\n",
    "\n",
    "        if action_mask is not None:\n",
    "            # Set masked action logits to very large negative value\n",
    "            # This makes their probability effectively 0 after softmax\n",
    "            action_logits = th.where(\n",
    "                action_mask,\n",
    "                th.tensor(float(\"-inf\"), device=action_logits.device, dtype=action_logits.dtype),\n",
    "                action_logits,\n",
    "            )\n",
    "\n",
    "        self.distribution = th.distributions.Categorical(logits=action_logits)\n",
    "        return self\n",
    "\n",
    "    def log_prob(self, actions: th.Tensor) -> th.Tensor:\n",
    "        \"\"\"Get log probability of actions.\"\"\"\n",
    "        return self.distribution.log_prob(actions)\n",
    "\n",
    "    def entropy(self) -> th.Tensor:\n",
    "        \"\"\"Get entropy of the distribution.\"\"\"\n",
    "        return self.distribution.entropy()\n",
    "\n",
    "    def sample(self) -> th.Tensor:\n",
    "        \"\"\"Sample an action from the distribution.\"\"\"\n",
    "        return self.distribution.sample()\n",
    "\n",
    "    def mode(self) -> th.Tensor:\n",
    "        \"\"\"Return the most likely action (greedy).\"\"\"\n",
    "        return th.argmax(self.distribution.probs, dim=-1)\n",
    "\n",
    "    def actions_from_params(\n",
    "        self,\n",
    "        action_logits: th.Tensor,\n",
    "        action_mask: Optional[th.Tensor] = None,\n",
    "        deterministic: bool = False,\n",
    "    ) -> th.Tensor:\n",
    "        \"\"\"\n",
    "        Sample actions from the distribution.\n",
    "\n",
    "        Args:\n",
    "            action_logits: Logits for each action\n",
    "            action_mask: Boolean mask where True = invalid\n",
    "            deterministic: If True, return mode instead of sampling\n",
    "        \"\"\"\n",
    "        self.proba_distribution(action_logits, action_mask)\n",
    "        if deterministic:\n",
    "            return self.mode()\n",
    "        return self.sample()\n",
    "\n",
    "    def log_prob_from_params(\n",
    "        self,\n",
    "        action_logits: th.Tensor,\n",
    "        action_mask: Optional[th.Tensor] = None,\n",
    "    ) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        Sample actions and get their log probabilities.\n",
    "\n",
    "        Returns:\n",
    "            actions: Sampled actions\n",
    "            log_probs: Log probabilities of sampled actions\n",
    "        \"\"\"\n",
    "        actions = self.actions_from_params(action_logits, action_mask)\n",
    "        log_prob = self.log_prob(actions)\n",
    "        return actions, log_prob\n",
    "\n",
    "\n",
    "class MaskedActorCriticPolicy(ActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Actor-Critic policy with action masking support for PPO.\n",
    "\n",
    "    This policy can handle dynamic action masks provided through the info dict.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Schedule,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # We'll override the action distribution\n",
    "        self.use_action_masking = True\n",
    "        super().__init__(observation_space, action_space, lr_schedule, **kwargs)\n",
    "\n",
    "    def _build(self, lr_schedule: Schedule) -> None:\n",
    "        \"\"\"\n",
    "        Create the networks and the optimizer.\n",
    "\n",
    "        Override to use masked distribution.\n",
    "        \"\"\"\n",
    "        super()._build(lr_schedule)\n",
    "\n",
    "        # Replace the action distribution with our masked version\n",
    "        if isinstance(self.action_space, spaces.Discrete):\n",
    "            self.action_dist = MaskedCategoricalDistribution(self.action_space.n)\n",
    "            self.action_net = self.action_dist.proba_distribution_net(self.mlp_extractor.latent_dim_pi)\n",
    "\n",
    "    def _get_action_dist_from_latent(\n",
    "        self,\n",
    "        latent_pi: th.Tensor,\n",
    "        action_masks: Optional[th.Tensor] = None,\n",
    "    ) -> MaskedCategoricalDistribution:\n",
    "        \"\"\"\n",
    "        Get action distribution from latent policy representation.\n",
    "\n",
    "        Args:\n",
    "            latent_pi: Latent representation from policy network\n",
    "            action_masks: Optional action masks (True = invalid)\n",
    "        \"\"\"\n",
    "        action_logits = self.action_net(latent_pi)\n",
    "        # Create a new distribution instance with the logits and masks\n",
    "        dist = MaskedCategoricalDistribution(self.action_space.n)\n",
    "        return dist.proba_distribution(action_logits, action_masks)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        obs: th.Tensor,\n",
    "        deterministic: bool = False,\n",
    "        action_masks: Optional[th.Tensor] = None,\n",
    "    ) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass in all networks (actor and critic).\n",
    "\n",
    "        Args:\n",
    "            obs: Observation\n",
    "            deterministic: Whether to sample or use deterministic actions\n",
    "            action_masks: Action masks (True = invalid)\n",
    "\n",
    "        Returns:\n",
    "            actions: Selected actions\n",
    "            values: Estimated values\n",
    "            log_probs: Log probability of actions\n",
    "        \"\"\"\n",
    "        # Preprocess the observation if needed\n",
    "        features = self.extract_features(obs)\n",
    "        if self.share_features_extractor:\n",
    "            latent_pi, latent_vf = self.mlp_extractor(features)\n",
    "        else:\n",
    "            pi_features, vf_features = features\n",
    "            latent_pi = self.mlp_extractor.forward_actor(pi_features)\n",
    "            latent_vf = self.mlp_extractor.forward_critic(vf_features)\n",
    "\n",
    "        # Evaluate the values for the given observations\n",
    "        values = self.value_net(latent_vf)\n",
    "\n",
    "        # Get action distribution with masking\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi, action_masks)\n",
    "\n",
    "        # Sample actions\n",
    "        if deterministic:\n",
    "            actions = distribution.mode()\n",
    "        else:\n",
    "            actions = distribution.sample()\n",
    "\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        actions = actions.reshape((-1,) + self.action_space.shape)\n",
    "\n",
    "        return actions, values, log_prob\n",
    "\n",
    "    def evaluate_actions(\n",
    "        self,\n",
    "        obs: th.Tensor,\n",
    "        actions: th.Tensor,\n",
    "        action_masks: Optional[th.Tensor] = None,\n",
    "    ) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluate actions according to the current policy.\n",
    "\n",
    "        Args:\n",
    "            obs: Observation\n",
    "            actions: Actions to evaluate\n",
    "            action_masks: Action masks (True = invalid)\n",
    "\n",
    "        Returns:\n",
    "            values: Estimated values\n",
    "            log_probs: Log probability of actions\n",
    "            entropy: Entropy of the distribution\n",
    "        \"\"\"\n",
    "        # Preprocess the observation if needed\n",
    "        features = self.extract_features(obs)\n",
    "        if self.share_features_extractor:\n",
    "            latent_pi, latent_vf = self.mlp_extractor(features)\n",
    "        else:\n",
    "            pi_features, vf_features = features\n",
    "            latent_pi = self.mlp_extractor.forward_actor(pi_features)\n",
    "            latent_vf = self.mlp_extractor.forward_critic(vf_features)\n",
    "\n",
    "        # Evaluate values\n",
    "        values = self.value_net(latent_vf)\n",
    "\n",
    "        # Get action distribution with masking\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi, action_masks)\n",
    "\n",
    "        # Evaluate log probabilities and entropy\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        entropy = distribution.entropy()\n",
    "\n",
    "        return values, log_prob, entropy\n",
    "\n",
    "    def predict_values(\n",
    "        self,\n",
    "        obs: th.Tensor,\n",
    "    ) -> th.Tensor:\n",
    "        \"\"\"\n",
    "        Get the estimated values according to the current policy.\n",
    "\n",
    "        Args:\n",
    "            obs: Observation\n",
    "\n",
    "        Returns:\n",
    "            Estimated values\n",
    "        \"\"\"\n",
    "        features = self.extract_features(obs)\n",
    "        if self.share_features_extractor:\n",
    "            _, latent_vf = self.mlp_extractor(features)\n",
    "        else:\n",
    "            _, vf_features = features\n",
    "            latent_vf = self.mlp_extractor.forward_critic(vf_features)\n",
    "        return self.value_net(latent_vf)\n",
    "\n",
    "    def _predict(\n",
    "        self,\n",
    "        observation: th.Tensor,\n",
    "        deterministic: bool = False,\n",
    "        action_masks: Optional[th.Tensor] = None,\n",
    "    ) -> th.Tensor:\n",
    "        \"\"\"\n",
    "        Get the action according to the policy for a given observation.\n",
    "\n",
    "        Args:\n",
    "            observation: Observation\n",
    "            deterministic: Whether to use stochastic or deterministic actions\n",
    "            action_masks: Action masks (True = invalid)\n",
    "\n",
    "        Returns:\n",
    "            Taken action\n",
    "        \"\"\"\n",
    "        actions, _, _ = self.forward(observation, deterministic, action_masks)\n",
    "        return actions\n",
    "\n",
    "\n",
    "class MaskedPPOPolicy(MaskedActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Policy class for PPO with action masking.\n",
    "    Alias for MaskedActorCriticPolicy for consistency with SB3 naming.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8a546ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MaskedCategoricalDistribution...\n",
      "Sampled actions: tensor([ 1, 12, 15, 34])\n",
      "All actions < 50: True\n",
      "Log probs shape: torch.Size([4])\n",
      "Entropy shape: torch.Size([4])\n",
      "\n",
      "Masked distribution test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test the masked distribution\n",
    "print(\"Testing MaskedCategoricalDistribution...\")\n",
    "\n",
    "n_actions = 100\n",
    "batch_size = 4\n",
    "\n",
    "dist = MaskedCategoricalDistribution(n_actions)\n",
    "\n",
    "# Create some dummy logits\n",
    "logits = th.randn(batch_size, n_actions)\n",
    "\n",
    "# Create a mask: first half of actions are valid, second half invalid\n",
    "mask = th.zeros(batch_size, n_actions, dtype=th.bool)\n",
    "mask[:, 50:] = True  # Mask second half\n",
    "\n",
    "# Test distribution\n",
    "dist.proba_distribution(logits, mask)\n",
    "\n",
    "# Sample actions\n",
    "actions = dist.sample()\n",
    "print(f\"Sampled actions: {actions}\")\n",
    "print(f\"All actions < 50: {th.all(actions < 50)}\")  # Should be True\n",
    "\n",
    "# Test log probs\n",
    "log_probs = dist.log_prob(actions)\n",
    "print(f\"Log probs shape: {log_probs.shape}\")\n",
    "\n",
    "# Test entropy\n",
    "entropy = dist.entropy()\n",
    "print(f\"Entropy shape: {entropy.shape}\")\n",
    "\n",
    "print(\"\\nMasked distribution test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae6d05",
   "metadata": {},
   "source": [
    "##### Action space definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd648867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action space definition\n",
    "class AdaptiveSteeringActionSpace:\n",
    "    \"\"\"\n",
    "    Manages adaptive action masking for steering control with pruning.\n",
    "\n",
    "    The action space is a 1D grid over steering angles.\n",
    "    Actions are gradually unmasked based on advantages, and low-performing actions are pruned.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        steering_range: Tuple[float, float] = (-np.pi/4, np.pi/4),\n",
    "        final_grid_size: int = 20,\n",
    "        initial_active_actions: int = 5,\n",
    "        unmask_rate: float = 0.2,  # Fraction of masked actions to unmask per update\n",
    "        prune_rate: float = 0.1,  # Fraction of active actions to prune per update\n",
    "        advantage_threshold: float = 0.0,  # Minimum advantage to trigger unmasking\n",
    "        min_samples_for_pruning: int = 50,  # Minimum samples before considering pruning\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            steering_range: (min, max) steering angle in radians\n",
    "            final_grid_size: Total number of discrete steering angles\n",
    "            initial_active_actions: Number of actions initially active\n",
    "            unmask_rate: Fraction of masked actions to unmask per update\n",
    "            prune_rate: Fraction of active actions to consider for pruning per update\n",
    "            advantage_threshold: Minimum advantage for an action to trigger unmasking neighbors\n",
    "            min_samples_for_pruning: Minimum times an action must be sampled before pruning\n",
    "        \"\"\"\n",
    "        self.steering_range = steering_range\n",
    "        self.final_grid_size = final_grid_size\n",
    "        self.initial_active_actions = initial_active_actions\n",
    "        self.unmask_rate = unmask_rate\n",
    "        self.prune_rate = prune_rate\n",
    "        self.advantage_threshold = advantage_threshold\n",
    "        self.min_samples_for_pruning = min_samples_for_pruning\n",
    "\n",
    "        # Create the full action space\n",
    "        self.n_actions = final_grid_size\n",
    "        self.steering_values = np.linspace(steering_range[0], steering_range[1], final_grid_size)\n",
    "\n",
    "        # Initialize mask: True = masked (invalid), False = unmasked (valid)\n",
    "        self.action_mask = np.ones(self.n_actions, dtype=bool)\n",
    "\n",
    "        # Initialize with sparse uniform grid\n",
    "        self._initialize_sparse_grid()\n",
    "\n",
    "        # Track action statistics\n",
    "        self.action_advantages = np.zeros(self.n_actions)\n",
    "        self.action_counts = np.zeros(self.n_actions)\n",
    "\n",
    "    def _initialize_sparse_grid(self):\n",
    "        \"\"\"Initialize with a uniform sparse grid matching initial_active_actions.\"\"\"\n",
    "        if self.initial_active_actions >= self.final_grid_size:\n",
    "            # If we want all actions initially, unmask everything\n",
    "            self.action_mask[:] = False\n",
    "        else:\n",
    "            # Distribute initial actions uniformly\n",
    "            indices = np.linspace(0, self.final_grid_size - 1, self.initial_active_actions, dtype=int)\n",
    "            self.action_mask[indices] = False\n",
    "\n",
    "    def get_valid_actions(self) -> np.ndarray:\n",
    "        \"\"\"Returns array of valid (unmasked) action indices.\"\"\"\n",
    "        return np.where(~self.action_mask)[0]\n",
    "\n",
    "    def get_action_mask(self) -> np.ndarray:\n",
    "        \"\"\"Returns boolean mask where True = invalid/masked.\"\"\"\n",
    "        return self.action_mask.copy()\n",
    "\n",
    "    def get_steering_value(self, action_idx: int) -> float:\n",
    "        \"\"\"Convert action index to steering angle value.\"\"\"\n",
    "        # Handle numpy arrays\n",
    "        if isinstance(action_idx, np.ndarray):\n",
    "            action_idx = int(action_idx.item())\n",
    "        else:\n",
    "            action_idx = int(action_idx)\n",
    "\n",
    "        return self.steering_values[action_idx]\n",
    "\n",
    "    def update_advantages(self, action_indices: np.ndarray, advantages: np.ndarray):\n",
    "        \"\"\"\n",
    "        Update running statistics for action advantages.\n",
    "\n",
    "        Args:\n",
    "            action_indices: Array of action indices taken\n",
    "            advantages: Corresponding advantage estimates\n",
    "        \"\"\"\n",
    "        action_indices = action_indices.astype(int).flatten()\n",
    "        advantages = advantages.flatten()\n",
    "\n",
    "        for action_idx, advantage in zip(action_indices, advantages):\n",
    "            self.action_advantages[action_idx] += advantage\n",
    "            self.action_counts[action_idx] += 1\n",
    "\n",
    "    def get_average_advantages(self) -> np.ndarray:\n",
    "        \"\"\"Get average advantage for each action (0 if never taken).\"\"\"\n",
    "        avg_advantages = np.zeros(self.n_actions)\n",
    "        valid = self.action_counts > 0\n",
    "        avg_advantages[valid] = self.action_advantages[valid] / self.action_counts[valid]\n",
    "        return avg_advantages\n",
    "\n",
    "    def _get_neighbors(self, action_idx: int) -> List[int]:\n",
    "        \"\"\"Get neighboring action indices (left and right).\"\"\"\n",
    "        neighbors = []\n",
    "        # Left neighbor\n",
    "        if action_idx > 0:\n",
    "            neighbors.append(action_idx - 1)\n",
    "        # Right neighbor\n",
    "        if action_idx < self.n_actions - 1:\n",
    "            neighbors.append(action_idx + 1)\n",
    "        return neighbors\n",
    "\n",
    "    def unmask_and_prune(self):\n",
    "        \"\"\"\n",
    "        Gradually unmask new actions near high-advantage actions,\n",
    "        and prune low-performing actions to maintain constant active action count.\n",
    "        \"\"\"\n",
    "        target_active = self.initial_active_actions\n",
    "        current_active = self.get_num_valid_actions()\n",
    "\n",
    "        avg_advantages = self.get_average_advantages()\n",
    "        valid_actions = self.get_valid_actions()\n",
    "\n",
    "        # Step 1: Identify candidates for pruning (low advantage, sufficient samples)\n",
    "        prune_candidates = []\n",
    "        for action_idx in valid_actions:\n",
    "            if self.action_counts[action_idx] >= self.min_samples_for_pruning:\n",
    "                prune_candidates.append((action_idx, avg_advantages[action_idx]))\n",
    "\n",
    "        # Step 2: Identify candidates for unmasking (neighbors of high-advantage actions)\n",
    "        unmask_candidates = set()\n",
    "\n",
    "        # Find high-advantage actions\n",
    "        high_advantage_actions = []\n",
    "        for action_idx in valid_actions:\n",
    "            if avg_advantages[action_idx] > self.advantage_threshold:\n",
    "                high_advantage_actions.append(action_idx)\n",
    "\n",
    "        if len(high_advantage_actions) == 0 and len(valid_actions) > 0:\n",
    "            # If no high-advantage actions, use the best available\n",
    "            best_action = valid_actions[np.argmax(avg_advantages[valid_actions])]\n",
    "            high_advantage_actions = [best_action]\n",
    "\n",
    "        # Collect masked neighbors of high-advantage actions\n",
    "        for action_idx in high_advantage_actions:\n",
    "            neighbors = self._get_neighbors(action_idx)\n",
    "            for neighbor_idx in neighbors:\n",
    "                if self.action_mask[neighbor_idx]:  # Currently masked\n",
    "                    unmask_candidates.add(neighbor_idx)\n",
    "\n",
    "        # Step 3: Determine how many to unmask and prune\n",
    "        n_to_unmask = int(len(unmask_candidates) * self.unmask_rate) if len(unmask_candidates) > 0 else 0\n",
    "        n_to_unmask = max(1, min(n_to_unmask, len(unmask_candidates)))\n",
    "\n",
    "        # We want to maintain approximately target_active actions\n",
    "        # If we unmask n_to_unmask, we should prune the same amount (or prune first then unmask)\n",
    "        n_to_prune = 0\n",
    "        if len(prune_candidates) > 0:\n",
    "            # Prune roughly the same number we're unmasking to maintain constant L1 norm\n",
    "            n_to_prune = min(n_to_unmask, int(len(prune_candidates) * self.prune_rate))\n",
    "            n_to_prune = max(0, n_to_prune)\n",
    "            # Don't prune so much that we go below target\n",
    "            n_to_prune = min(n_to_prune, current_active - target_active + n_to_unmask)\n",
    "\n",
    "        # Step 4: Prune low-performing actions first\n",
    "        if n_to_prune > 0 and len(prune_candidates) > 0:\n",
    "            # Sort by advantage (lowest first)\n",
    "            prune_candidates.sort(key=lambda x: x[1])\n",
    "            to_prune = [action_idx for action_idx, _ in prune_candidates[:n_to_prune]]\n",
    "\n",
    "            for action_idx in to_prune:\n",
    "                self.action_mask[action_idx] = True  # Mask (prune) the action\n",
    "\n",
    "        # Step 5: Unmask new actions\n",
    "        if n_to_unmask > 0 and len(unmask_candidates) > 0:\n",
    "            unmask_list = list(unmask_candidates)\n",
    "            to_unmask = np.random.choice(unmask_list, size=min(n_to_unmask, len(unmask_list)), replace=False)\n",
    "\n",
    "            for action_idx in to_unmask:\n",
    "                self.action_mask[action_idx] = False  # Unmask\n",
    "\n",
    "    def get_num_valid_actions(self) -> int:\n",
    "        \"\"\"Returns the current number of valid (unmasked) actions.\"\"\"\n",
    "        return np.sum(~self.action_mask)\n",
    "\n",
    "    def get_progress(self) -> float:\n",
    "        \"\"\"Returns the fraction of actions that have been explored (0 to 1).\"\"\"\n",
    "        # Progress isn't just about unmasked count, but about exploration\n",
    "        # We define progress as the fraction of actions that have been sampled at least once\n",
    "        return np.sum(self.action_counts > 0) / self.n_actions\n",
    "\n",
    "    def reset_statistics(self):\n",
    "        \"\"\"Reset advantage and count statistics (use if needed for episodic tracking).\"\"\"\n",
    "        self.action_advantages = np.zeros(self.n_actions)\n",
    "        self.action_counts = np.zeros(self.n_actions)\n",
    "\n",
    "\n",
    "class AdaptiveSteeringActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Gym wrapper that converts the adaptive discrete steering actions to continuous steering\n",
    "    for highway-env, while handling action masking.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        adaptive_action_space: AdaptiveSteeringActionSpace,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: Highway-env environment configured with ContinuousAction (steering only)\n",
    "            adaptive_action_space: AdaptiveSteeringActionSpace instance managing the masking\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.adaptive_space = adaptive_action_space\n",
    "\n",
    "        # Override the action space to be discrete with the full action count\n",
    "        self.action_space = gym.spaces.Discrete(self.adaptive_space.n_actions)\n",
    "\n",
    "    def step(self, action: int):\n",
    "        \"\"\"Convert discrete action to continuous steering and step environment.\"\"\"\n",
    "        # Convert discrete action to steering angle\n",
    "        steering = self.adaptive_space.get_steering_value(action)\n",
    "\n",
    "        # Map to [-1, 1] range expected by ContinuousAction\n",
    "        steering_norm = self._map_to_normalized(steering, self.adaptive_space.steering_range)\n",
    "\n",
    "        # For steering-only control, we use 0 acceleration (let the car maintain speed)\n",
    "        continuous_action = np.array([steering_norm])\n",
    "        return self.env.step(continuous_action)\n",
    "\n",
    "    def _map_to_normalized(self, value: float, range_: Tuple[float, float]) -> float:\n",
    "        \"\"\"Map a value from a range to [-1, 1].\"\"\"\n",
    "        return 2 * (value - range_[0]) / (range_[1] - range_[0]) - 1\n",
    "\n",
    "    def get_action_mask(self) -> np.ndarray:\n",
    "        \"\"\"Get current action mask for the policy.\"\"\"\n",
    "        return self.adaptive_space.get_action_mask()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78601439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total actions: 20\n",
      "Initial valid actions: 5\n",
      "Valid action indices: [ 0  4  9 14 19]\n",
      "Steering values: [np.float64(-0.7853981633974483), np.float64(-0.4547041998616806), np.float64(-0.04133674544197097), np.float64(0.3720307089777386), np.float64(0.7853981633974483)]\n",
      "\n",
      "Average advantages computed\n",
      "\n",
      "Before unmask/prune: 5 valid actions\n",
      "After unmask/prune: 6 valid actions\n",
      "Valid action indices: [ 0  4  9 14 18 19]\n",
      "Progress: 25.00%\n"
     ]
    }
   ],
   "source": [
    "# Test adaptive speering space\n",
    "adaptive_space = AdaptiveSteeringActionSpace(\n",
    "    steering_range=(-np.pi/4, np.pi/4),\n",
    "    final_grid_size=20,\n",
    "    initial_active_actions=5,\n",
    "    unmask_rate=0.2,\n",
    "    prune_rate=0.1,\n",
    ")\n",
    "\n",
    "print(f\"Total actions: {adaptive_space.n_actions}\")\n",
    "print(f\"Initial valid actions: {adaptive_space.get_num_valid_actions()}\")\n",
    "print(f\"Valid action indices: {adaptive_space.get_valid_actions()}\")\n",
    "print(f\"Steering values: {[adaptive_space.get_steering_value(i) for i in adaptive_space.get_valid_actions()]}\")\n",
    "\n",
    "# Simulate some advantages\n",
    "valid_actions = adaptive_space.get_valid_actions()\n",
    "action_indices = np.random.choice(valid_actions, size=100)\n",
    "advantages = np.random.randn(100)\n",
    "\n",
    "adaptive_space.update_advantages(action_indices, advantages)\n",
    "print(f\"\\nAverage advantages computed\")\n",
    "\n",
    "# Unmask and prune\n",
    "print(f\"\\nBefore unmask/prune: {adaptive_space.get_num_valid_actions()} valid actions\")\n",
    "adaptive_space.unmask_and_prune()\n",
    "print(f\"After unmask/prune: {adaptive_space.get_num_valid_actions()} valid actions\")\n",
    "print(f\"Valid action indices: {adaptive_space.get_valid_actions()}\")\n",
    "print(f\"Progress: {adaptive_space.get_progress():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ffcac9",
   "metadata": {},
   "source": [
    "##### Zooming Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb47d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zooming PPO implementation\n",
    "SelfAdaptivePPO = TypeVar(\"SelfAdaptivePPO\", bound=\"AdaptivePPO\")\n",
    "\n",
    "\n",
    "class AdaptiveRolloutBuffer(RolloutBuffer):\n",
    "    \"\"\"\n",
    "    Extended rollout buffer that also stores action masks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.action_masks = None\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        super().reset()\n",
    "        if self.action_masks is not None:\n",
    "            self.action_masks = np.zeros(\n",
    "                (self.buffer_size, self.n_envs, self.action_masks.shape[-1]),\n",
    "                dtype=np.bool_,\n",
    "            )\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        *args,\n",
    "        action_masks: Optional[np.ndarray] = None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Add elements to the buffer, including action masks.\n",
    "        \"\"\"\n",
    "        # Initialize action_masks buffer on first add if provided\n",
    "        if action_masks is not None and self.action_masks is None:\n",
    "            self.action_masks = np.zeros(\n",
    "                (self.buffer_size, self.n_envs, action_masks.shape[-1]),\n",
    "                dtype=np.bool_,\n",
    "            )\n",
    "\n",
    "        # Store action masks\n",
    "        if action_masks is not None:\n",
    "            self.action_masks[self.pos] = action_masks.copy()\n",
    "\n",
    "        # Call parent add\n",
    "        super().add(*args, **kwargs)\n",
    "\n",
    "    # Don't override get() - just use parent implementation\n",
    "    # We'll handle action masks separately in the training loop\n",
    "\n",
    "\n",
    "class AdaptivePPO(PPO):\n",
    "    \"\"\"\n",
    "    PPO with adaptive action space unmasking.\n",
    "\n",
    "    This algorithm gradually unmasks new actions during training based on\n",
    "    advantage estimates, allowing the agent to explore finer-grained actions\n",
    "    as it learns.\n",
    "    \"\"\"\n",
    "\n",
    "    policy_aliases: ClassVar[Dict[str, Type[MaskedPPOPolicy]]] = {\n",
    "        \"MaskedMlpPolicy\": MaskedPPOPolicy,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: Union[str, Type[MaskedPPOPolicy]],\n",
    "        env: GymEnv,\n",
    "        adaptive_action_space: AdaptiveSteeringActionSpace,\n",
    "        unmask_frequency: int = 10,  # Unmask every N policy updates\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            policy: Policy type or string alias\n",
    "            env: Environment\n",
    "            adaptive_action_space: AdaptiveActionSpace instance managing masking\n",
    "            unmask_frequency: How often to unmask new actions (in policy updates)\n",
    "            *args, **kwargs: Additional arguments for PPO\n",
    "        \"\"\"\n",
    "        self.adaptive_action_space = adaptive_action_space\n",
    "        self.unmask_frequency = unmask_frequency\n",
    "        self._num_policy_updates = 0\n",
    "\n",
    "        # Override default policy if using string\n",
    "        if isinstance(policy, str) and policy == \"MlpPolicy\":\n",
    "            policy = \"MaskedMlpPolicy\"\n",
    "\n",
    "        super().__init__(policy, env, *args, **kwargs)\n",
    "\n",
    "        # Track advantages per action for unmasking\n",
    "        self.action_advantages_buffer = []\n",
    "\n",
    "    @classmethod\n",
    "    def load(\n",
    "        cls,\n",
    "        path: str,\n",
    "        env: Optional[GymEnv] = None,\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        custom_objects: Optional[Dict[str, Any]] = None,\n",
    "        print_system_info: bool = False,\n",
    "        force_reset: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load the model from a zip-file.\n",
    "\n",
    "        This overrides the parent load() to handle the adaptive_action_space parameter.\n",
    "\n",
    "        Args:\n",
    "            path: Path to the file (or a file-like) where to load the agent from\n",
    "            env: Environment that the model was trained on. Must be provided.\n",
    "            device: Device on which the code should run\n",
    "            custom_objects: Dictionary of custom objects to replace on load\n",
    "            print_system_info: Whether to print system info\n",
    "            force_reset: Force a call to reset() before training\n",
    "            **kwargs: Additional keyword arguments (must include 'adaptive_action_space')\n",
    "\n",
    "        Returns:\n",
    "            Loaded model\n",
    "        \"\"\"\n",
    "        if env is None:\n",
    "            raise ValueError(\"env must be provided when loading AdaptivePPO\")\n",
    "\n",
    "        # Get adaptive_action_space from kwargs or custom_objects\n",
    "        if \"adaptive_action_space\" not in kwargs:\n",
    "            if custom_objects is not None and \"adaptive_action_space\" in custom_objects:\n",
    "                kwargs[\"adaptive_action_space\"] = custom_objects[\"adaptive_action_space\"]\n",
    "            else:\n",
    "                raise ValueError(\"adaptive_action_space must be provided when loading AdaptivePPO\")\n",
    "\n",
    "        adaptive_action_space = kwargs[\"adaptive_action_space\"]\n",
    "        unmask_frequency = kwargs.get(\"unmask_frequency\", 10)\n",
    "\n",
    "        # Load as a regular PPO model first (just to get the saved data)\n",
    "        import zipfile\n",
    "        import json\n",
    "        import pathlib\n",
    "\n",
    "        # Add .zip extension if not present\n",
    "        if not path.endswith(\".zip\"):\n",
    "            path = path + \".zip\"\n",
    "\n",
    "        # Load the data from the zip file\n",
    "        with zipfile.ZipFile(path, \"r\") as archive:\n",
    "            # Load the parameters\n",
    "            data_json = archive.read(\"data\").decode(\"utf-8\")\n",
    "            data = json.loads(data_json)\n",
    "\n",
    "        # Get hyperparameters from saved data if available\n",
    "        learning_rate = data.get(\"learning_rate\", 5e-4)\n",
    "        n_steps = data.get(\"n_steps\", 128)\n",
    "        batch_size = data.get(\"batch_size\", 64)\n",
    "        n_epochs = data.get(\"n_epochs\", 10)\n",
    "        gamma = data.get(\"gamma\", 0.9)\n",
    "        gae_lambda = data.get(\"gae_lambda\", 0.95)\n",
    "\n",
    "        # Create a new model with the adaptive_action_space\n",
    "        model = cls(\n",
    "            policy=\"MaskedMlpPolicy\",\n",
    "            env=env,\n",
    "            adaptive_action_space=adaptive_action_space,\n",
    "            unmask_frequency=unmask_frequency,\n",
    "            learning_rate=learning_rate,\n",
    "            n_steps=n_steps,\n",
    "            batch_size=batch_size,\n",
    "            n_epochs=n_epochs,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        # Now load the parameters (network weights)\n",
    "        model.set_parameters(load_path_or_dict=path, device=device)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _setup_model(self) -> None:\n",
    "        \"\"\"Setup the model and replace rollout buffer with adaptive version.\"\"\"\n",
    "        super()._setup_model()\n",
    "\n",
    "        # Replace rollout buffer with adaptive version\n",
    "        self.rollout_buffer = AdaptiveRolloutBuffer(\n",
    "            self.n_steps,\n",
    "            self.observation_space,\n",
    "            self.action_space,\n",
    "            device=self.device,\n",
    "            gamma=self.gamma,\n",
    "            gae_lambda=self.gae_lambda,\n",
    "            n_envs=self.n_envs,\n",
    "        )\n",
    "\n",
    "    def collect_rollouts(\n",
    "        self,\n",
    "        env,\n",
    "        callback,\n",
    "        rollout_buffer,\n",
    "        n_rollout_steps: int,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Collect experiences using the current policy and fill the rollout buffer.\n",
    "        Modified to include action masks.\n",
    "        \"\"\"\n",
    "        assert self._last_obs is not None, \"No previous observation was provided\"\n",
    "\n",
    "        # Switch to eval mode\n",
    "        self.policy.set_training_mode(False)\n",
    "\n",
    "        n_steps = 0\n",
    "        rollout_buffer.reset()\n",
    "\n",
    "        callback.on_rollout_start()\n",
    "\n",
    "        while n_steps < n_rollout_steps:\n",
    "            with th.no_grad():\n",
    "                # Convert to pytorch tensor\n",
    "                obs_tensor = obs_as_tensor(self._last_obs, self.device)\n",
    "\n",
    "                # Get current action mask\n",
    "                action_mask = self.adaptive_action_space.get_action_mask()\n",
    "                # Expand mask for each environment\n",
    "                action_mask_batch = np.tile(action_mask, (self.n_envs, 1))\n",
    "                action_mask_tensor = th.as_tensor(action_mask_batch, device=self.device)\n",
    "\n",
    "                # Get actions with masking\n",
    "                actions, values, log_probs = self.policy.forward(\n",
    "                    obs_tensor,\n",
    "                    deterministic=False,\n",
    "                    action_masks=action_mask_tensor,\n",
    "                )\n",
    "\n",
    "            actions = actions.cpu().numpy()\n",
    "\n",
    "            # Rescale and perform action\n",
    "            clipped_actions = actions\n",
    "\n",
    "            # Clip the actions to avoid out of bound error\n",
    "            from gymnasium import spaces as gym_spaces\n",
    "            if isinstance(self.action_space, gym_spaces.Box):\n",
    "                clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
    "\n",
    "            new_obs, rewards, dones, infos = env.step(clipped_actions)\n",
    "\n",
    "            self.num_timesteps += self.n_envs\n",
    "\n",
    "            # Give access to local variables\n",
    "            callback.update_locals(locals())\n",
    "            if not callback.on_step():\n",
    "                return False\n",
    "\n",
    "            self._update_info_buffer(infos, dones)\n",
    "            n_steps += 1\n",
    "\n",
    "            if isinstance(self.action_space, gym_spaces.Discrete):\n",
    "                # Reshape in case of discrete action\n",
    "                actions = actions.reshape(-1, 1)\n",
    "\n",
    "            # Handle timeout by bootstrapping with value function\n",
    "            for idx, done in enumerate(dones):\n",
    "                if (\n",
    "                    done\n",
    "                    and infos[idx].get(\"terminal_observation\") is not None\n",
    "                    and infos[idx].get(\"TimeLimit.truncated\", False)\n",
    "                ):\n",
    "                    terminal_obs = self.policy.obs_to_tensor(infos[idx][\"terminal_observation\"])[0]\n",
    "                    with th.no_grad():\n",
    "                        terminal_value = self.policy.predict_values(terminal_obs)[0]\n",
    "                    rewards[idx] += self.gamma * terminal_value\n",
    "\n",
    "            # Add to buffer with action masks\n",
    "            rollout_buffer.add(\n",
    "                self._last_obs,\n",
    "                actions,\n",
    "                rewards,\n",
    "                self._last_episode_starts,\n",
    "                values,\n",
    "                log_probs,\n",
    "                action_masks=action_mask_batch,\n",
    "            )\n",
    "            self._last_obs = new_obs\n",
    "            self._last_episode_starts = dones\n",
    "\n",
    "        # Compute value for the last timestep\n",
    "        with th.no_grad():\n",
    "            obs_tensor = obs_as_tensor(new_obs, self.device)\n",
    "            values = self.policy.predict_values(obs_tensor)\n",
    "\n",
    "        rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)\n",
    "\n",
    "        callback.on_rollout_end()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Update policy using the currently gathered rollout buffer.\n",
    "        Modified to use action masks and track advantages for unmasking.\n",
    "        \"\"\"\n",
    "        # Switch to train mode\n",
    "        self.policy.set_training_mode(True)\n",
    "\n",
    "        # Update optimizer learning rate\n",
    "        self._update_learning_rate(self.policy.optimizer)\n",
    "\n",
    "        # Compute current clip range\n",
    "        clip_range = self.clip_range(self._current_progress_remaining)\n",
    "\n",
    "        # Optional: clip range for the value function\n",
    "        if self.clip_range_vf is not None:\n",
    "            clip_range_vf = self.clip_range_vf(self._current_progress_remaining)\n",
    "\n",
    "        entropy_losses = []\n",
    "        pg_losses, value_losses = [], []\n",
    "        clip_fractions = []\n",
    "\n",
    "        continue_training = True\n",
    "\n",
    "        # Track advantages for unmasking\n",
    "        all_actions = []\n",
    "        all_advantages = []\n",
    "\n",
    "        # Train for n_epochs epochs\n",
    "        for epoch in range(self.n_epochs):\n",
    "            approx_kl_divs = []\n",
    "\n",
    "            # Do a complete pass on the rollout buffer\n",
    "            for rollout_data in self.rollout_buffer.get(self.batch_size):\n",
    "                actions = rollout_data.actions\n",
    "                observations = rollout_data.observations\n",
    "                old_values = rollout_data.old_values\n",
    "                old_log_prob = rollout_data.old_log_prob\n",
    "                advantages = rollout_data.advantages\n",
    "                returns = rollout_data.returns\n",
    "\n",
    "                # Get current action mask (same for all samples in this training iteration)\n",
    "                action_mask = self.adaptive_action_space.get_action_mask()\n",
    "                batch_size_current = observations.shape[0]\n",
    "                action_mask_batch = np.tile(action_mask, (batch_size_current, 1))\n",
    "                action_masks = th.as_tensor(action_mask_batch, device=self.device)\n",
    "\n",
    "                # Store for unmasking\n",
    "                all_actions.append(actions.cpu().numpy())\n",
    "                all_advantages.append(advantages.cpu().numpy())\n",
    "\n",
    "                from gymnasium import spaces as gym_spaces\n",
    "                if isinstance(self.action_space, gym_spaces.Discrete):\n",
    "                    # Convert discrete action from float to long\n",
    "                    actions = actions.long().flatten()\n",
    "\n",
    "                # Re-sample policy and value\n",
    "                values, log_prob, entropy = self.policy.evaluate_actions(\n",
    "                    observations,\n",
    "                    actions,\n",
    "                    action_masks=action_masks,\n",
    "                )\n",
    "                values = values.flatten()\n",
    "\n",
    "                # Normalize advantage\n",
    "                advantages = advantages.flatten()\n",
    "                if self.normalize_advantage and len(advantages) > 1:\n",
    "                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "                # Ratio between old and new policy\n",
    "                ratio = th.exp(log_prob - old_log_prob)\n",
    "\n",
    "                # Clipped surrogate loss\n",
    "                policy_loss_1 = advantages * ratio\n",
    "                policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
    "                policy_loss = -th.min(policy_loss_1, policy_loss_2).mean()\n",
    "\n",
    "                # Logging\n",
    "                pg_losses.append(policy_loss.item())\n",
    "                clip_fraction = th.mean((th.abs(ratio - 1) > clip_range).float()).item()\n",
    "                clip_fractions.append(clip_fraction)\n",
    "\n",
    "                if self.clip_range_vf is None:\n",
    "                    # No clipping\n",
    "                    values_pred = values\n",
    "                else:\n",
    "                    # Clip the difference between old and new value\n",
    "                    values_pred = old_values + th.clamp(\n",
    "                        values - old_values, -clip_range_vf, clip_range_vf\n",
    "                    )\n",
    "\n",
    "                # Value loss\n",
    "                value_loss = th.nn.functional.mse_loss(returns, values_pred)\n",
    "                value_losses.append(value_loss.item())\n",
    "\n",
    "                # Entropy loss favor exploration\n",
    "                if entropy is None:\n",
    "                    entropy_loss = -th.mean(-log_prob)\n",
    "                else:\n",
    "                    entropy_loss = -th.mean(entropy)\n",
    "\n",
    "                entropy_losses.append(entropy_loss.item())\n",
    "\n",
    "                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
    "\n",
    "                # Optimization step\n",
    "                self.policy.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip grad norm\n",
    "                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                self.policy.optimizer.step()\n",
    "                approx_kl_divs.append(th.mean(old_log_prob - log_prob).detach().cpu().numpy())\n",
    "\n",
    "            self._n_updates += 1\n",
    "            if not continue_training:\n",
    "                break\n",
    "\n",
    "        # Update adaptive action space based on advantages\n",
    "        self._num_policy_updates += 1\n",
    "        if self._num_policy_updates % self.unmask_frequency == 0:\n",
    "            self._update_action_space(all_actions, all_advantages)\n",
    "\n",
    "        # Log training metrics\n",
    "        explained_var = self._explained_variance(\n",
    "            self.rollout_buffer.values.flatten(),\n",
    "            self.rollout_buffer.returns.flatten(),\n",
    "        )\n",
    "\n",
    "        # Logging\n",
    "        self.logger.record(\"train/entropy_loss\", np.mean(entropy_losses))\n",
    "        self.logger.record(\"train/policy_gradient_loss\", np.mean(pg_losses))\n",
    "        self.logger.record(\"train/value_loss\", np.mean(value_losses))\n",
    "        self.logger.record(\"train/approx_kl\", np.mean(approx_kl_divs))\n",
    "        self.logger.record(\"train/clip_fraction\", np.mean(clip_fractions))\n",
    "        self.logger.record(\"train/loss\", loss.item())\n",
    "        self.logger.record(\"train/explained_variance\", explained_var)\n",
    "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "        self.logger.record(\"train/clip_range\", clip_range)\n",
    "        if self.clip_range_vf is not None:\n",
    "            self.logger.record(\"train/clip_range_vf\", clip_range_vf)\n",
    "\n",
    "        # Log adaptive action space stats\n",
    "        self.logger.record(\"adaptive/num_valid_actions\", self.adaptive_action_space.get_num_valid_actions())\n",
    "        self.logger.record(\"adaptive/progress\", self.adaptive_action_space.get_progress())\n",
    "\n",
    "    def _update_action_space(self, all_actions: list, all_advantages: list):\n",
    "        \"\"\"\n",
    "        Update the adaptive action space based on collected advantages.\n",
    "\n",
    "        Args:\n",
    "            all_actions: List of action batches\n",
    "            all_advantages: List of advantage batches\n",
    "        \"\"\"\n",
    "        # Concatenate all batches\n",
    "        actions = np.concatenate(all_actions, axis=0).flatten()\n",
    "        advantages = np.concatenate(all_advantages, axis=0).flatten()\n",
    "\n",
    "        # Update advantage statistics\n",
    "        self.adaptive_action_space.update_advantages(actions, advantages)\n",
    "\n",
    "        # Unmask new actions (and prune if the method exists)\n",
    "        if hasattr(self.adaptive_action_space, 'unmask_and_prune'):\n",
    "            # Steering-only version with pruning\n",
    "            self.adaptive_action_space.unmask_and_prune()\n",
    "        elif hasattr(self.adaptive_action_space, 'unmask_actions'):\n",
    "            # Original 2D version without pruning\n",
    "            self.adaptive_action_space.unmask_actions()\n",
    "        else:\n",
    "            raise AttributeError(\"Adaptive action space must have either unmask_and_prune() or unmask_actions() method\")\n",
    "\n",
    "        # Log progress\n",
    "        num_valid = self.adaptive_action_space.get_num_valid_actions()\n",
    "        progress = self.adaptive_action_space.get_progress()\n",
    "        print(f\"[Adaptive] Valid actions: {num_valid}/{self.adaptive_action_space.n_actions} ({progress:.1%})\")\n",
    "\n",
    "    def _explained_variance(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Computes fraction of variance that ypred explains about y.\n",
    "        Returns 1 - Var[y-ypred] / Var[y]\n",
    "        \"\"\"\n",
    "        var_y = np.var(y_true)\n",
    "        return 1 - np.var(y_true - y_pred) / (var_y + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f02b9",
   "metadata": {},
   "source": [
    "#### II. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53be85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment set up functions\n",
    "def make_adaptive_steering_env(rank=0, seed=0):\n",
    "    \"\"\"\n",
    "    Create a highway environment with adaptive discrete steering actions.\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        # Create environment with continuous action space (steering only)\n",
    "        env = gym.make(\n",
    "            \"highway-fast-v0\",\n",
    "            config={\n",
    "                \"action\": {\n",
    "                    \"type\": \"ContinuousAction\",\n",
    "                    \"steering_range\": [-np.pi/4, np.pi/4],\n",
    "                    \"longitudinal\": False,  # No throttle control\n",
    "                    \"lateral\": True,        # Steering only\n",
    "                },\n",
    "                \"duration\": 40,\n",
    "                \"policy_frequency\": 2,  # Control at 2 Hz\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Wrap with adaptive action space\n",
    "        adaptive_space = AdaptiveSteeringActionSpace(\n",
    "            steering_range=(-np.pi/4, np.pi/4),\n",
    "            final_grid_size=20,            # 20 possible steering angles\n",
    "            initial_active_actions=5,       # Start with 5 active\n",
    "            unmask_rate=0.2,\n",
    "            prune_rate=0.1,\n",
    "            advantage_threshold=0.0,\n",
    "        )\n",
    "\n",
    "        env = AdaptiveSteeringActionWrapper(env, adaptive_space)\n",
    "        env.reset(seed=seed + rank)\n",
    "        return env\n",
    "\n",
    "    return _init\n",
    "\n",
    "\n",
    "def make_baseline_steering_env(rank=0, seed=0):\n",
    "    \"\"\"\n",
    "    Create a baseline highway environment with standard discrete steering actions.\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = gym.make(\n",
    "            \"highway-fast-v0\",\n",
    "            config={\n",
    "                \"action\": {\n",
    "                    \"type\": \"DiscreteAction\",\n",
    "                    \"steering_range\": [-np.pi/4, np.pi/4],\n",
    "                    \"longitudinal\": False,      # No throttle control\n",
    "                    \"lateral\": True,            # Steering only\n",
    "                    \"actions_per_axis\": 5,      # 5 steering angles (same as initial adaptive)\n",
    "                },\n",
    "                \"duration\": 40,\n",
    "                \"policy_frequency\": 2,\n",
    "            }\n",
    "        )\n",
    "        env.reset(seed=seed + rank)\n",
    "        return env\n",
    "\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training functions\n",
    "def train_adaptive_steering_ppo(\n",
    "    total_timesteps=100_000,\n",
    "    n_envs=4,\n",
    "    save_path=\"highway_adaptive_steering_ppo\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train PPO with adaptive steering action space and pruning.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Training Adaptive Steering PPO with Pruning\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Create the adaptive action space (shared across all envs)\n",
    "    adaptive_space = AdaptiveSteeringActionSpace(\n",
    "        steering_range=(-np.pi/4, np.pi/4),\n",
    "        final_grid_size=20,\n",
    "        initial_active_actions=5,\n",
    "        unmask_rate=0.2,\n",
    "        prune_rate=0.1,\n",
    "        advantage_threshold=0.0,\n",
    "    )\n",
    "\n",
    "    # Create vectorized environments\n",
    "    def make_env_shared(rank):\n",
    "        def _init():\n",
    "            env = gym.make(\n",
    "                \"highway-fast-v0\",\n",
    "                config={\n",
    "                    \"action\": {\n",
    "                        \"type\": \"ContinuousAction\",\n",
    "                        \"steering_range\": [-np.pi/4, np.pi/4],\n",
    "                        \"longitudinal\": False,\n",
    "                        \"lateral\": True,\n",
    "                    },\n",
    "                    \"duration\": 40,\n",
    "                    \"policy_frequency\": 2,\n",
    "                }\n",
    "            )\n",
    "            env = AdaptiveSteeringActionWrapper(env, adaptive_space)\n",
    "            env.reset(seed=rank)\n",
    "            return env\n",
    "        return _init\n",
    "\n",
    "    env = DummyVecEnv([make_env_shared(i) for i in range(n_envs)])\n",
    "\n",
    "    # Create the model\n",
    "    model = AdaptivePPO(\n",
    "        \"MaskedMlpPolicy\",\n",
    "        env,\n",
    "        adaptive_action_space=adaptive_space,\n",
    "        unmask_frequency=10,  # Unmask/prune every 10 policy updates\n",
    "        n_steps=512 // n_envs,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        learning_rate=5e-4,\n",
    "        gamma=0.9,\n",
    "        verbose=1,\n",
    "        tensorboard_log=f\"{save_path}/\",\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    print(f\"\\nStarting training for {total_timesteps} timesteps...\")\n",
    "    print(f\"Initial valid actions: {adaptive_space.get_num_valid_actions()}/{adaptive_space.n_actions}\")\n",
    "    print(f\"Action space: Steering only (no acceleration)\")\n",
    "\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "    print(f\"\\nFinal valid actions: {adaptive_space.get_num_valid_actions()}/{adaptive_space.n_actions}\")\n",
    "    print(f\"Actions explored: {adaptive_space.get_progress():.1%}\")\n",
    "\n",
    "    # Save the model\n",
    "    model.save(f\"{save_path}/model\")\n",
    "    print(f\"\\nModel saved to {save_path}/model\")\n",
    "\n",
    "    return model, adaptive_space\n",
    "\n",
    "\n",
    "def train_baseline_steering_ppo(\n",
    "    total_timesteps=100_000,\n",
    "    n_envs=4,\n",
    "    save_path=\"highway_baseline_steering_ppo\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train baseline PPO with standard discrete steering actions.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Training Baseline Steering PPO (5 uniform actions)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Create vectorized environments\n",
    "    env = make_vec_env(\n",
    "        make_baseline_steering_env(),\n",
    "        n_envs=n_envs,\n",
    "        vec_env_cls=DummyVecEnv,\n",
    "    )\n",
    "\n",
    "    # Create the model\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        n_steps=512 // n_envs,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        learning_rate=5e-4,\n",
    "        gamma=0.9,\n",
    "        verbose=1,\n",
    "        tensorboard_log=f\"{save_path}/\",\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    print(f\"\\nStarting training for {total_timesteps} timesteps...\")\n",
    "    print(f\"Action space: 5 discrete steering angles (uniform)\")\n",
    "\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "    # Save the model\n",
    "    model.save(f\"{save_path}/model\")\n",
    "    print(f\"\\nModel saved to {save_path}/model\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "N_TIMESTEPS = 100_000\n",
    "N_ENVS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c593454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Zooming PPO\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ZOOMING STEERING PPO TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "adaptive_model, adaptive_space = train_adaptive_steering_ppo(\n",
    "    total_timesteps=N_TIMESTEPS,\n",
    "    n_envs=N_ENVS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Baseline PPO\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE STEERING PPO TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "baseline_model = train_baseline_steering_ppo(\n",
    "    total_timesteps=N_TIMESTEPS,\n",
    "    n_envs=N_ENVS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecaad6f",
   "metadata": {},
   "source": [
    "#### III. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model(\n",
    "    model,\n",
    "    env,\n",
    "    n_episodes=10,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    track_actions=False,\n",
    "    adaptive_space=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on the environment.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PPO or AdaptivePPO model\n",
    "        env: Evaluation environment\n",
    "        n_episodes: Number of episodes to run\n",
    "        deterministic: Use deterministic policy\n",
    "        render: Render the environment\n",
    "        track_actions: Track which actions are used\n",
    "        adaptive_space: AdaptiveActionSpace instance (for action tracking)\n",
    "\n",
    "    Returns:\n",
    "        dict: Evaluation statistics\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    collision_count = 0\n",
    "    success_count = 0\n",
    "    action_counts = Counter()\n",
    "\n",
    "    print(f\"\\nEvaluating for {n_episodes} episodes...\")\n",
    "    print(f\"Policy: {'Deterministic' if deterministic else 'Stochastic'}\")\n",
    "    print(f\"Rendering: {'Enabled' if render else 'Disabled'}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = truncated = False\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "\n",
    "        while not (done or truncated):\n",
    "            action, _ = model.predict(obs, deterministic=deterministic)\n",
    "\n",
    "            # Track action usage\n",
    "            if track_actions:\n",
    "                action_counts[int(action)] += 1\n",
    "\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        # Track episode outcome\n",
    "        if info.get(\"crashed\", False):\n",
    "            collision_count += 1\n",
    "        if episode_reward > 20:  # Heuristic for \"success\"\n",
    "            success_count += 1\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "\n",
    "        print(f\"Episode {ep+1:2d}: Reward = {episode_reward:6.2f}, Length = {episode_length:3d}\", end=\"\")\n",
    "        if info.get(\"crashed\", False):\n",
    "            print(\" [CRASHED]\")\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Compute statistics\n",
    "    results = {\n",
    "        \"mean_reward\": np.mean(episode_rewards),\n",
    "        \"std_reward\": np.std(episode_rewards),\n",
    "        \"min_reward\": np.min(episode_rewards),\n",
    "        \"max_reward\": np.max(episode_rewards),\n",
    "        \"mean_length\": np.mean(episode_lengths),\n",
    "        \"std_length\": np.std(episode_lengths),\n",
    "        \"collision_rate\": collision_count / n_episodes,\n",
    "        \"success_rate\": success_count / n_episodes,\n",
    "    }\n",
    "\n",
    "    # Action distribution analysis\n",
    "    if track_actions and len(action_counts) > 0:\n",
    "        results[\"unique_actions_used\"] = len(action_counts)\n",
    "        results[\"action_distribution\"] = dict(action_counts)\n",
    "\n",
    "        # Show action usage\n",
    "        print(f\"\\nAction Usage:\")\n",
    "        print(f\"  Unique actions used: {len(action_counts)}\")\n",
    "\n",
    "        if adaptive_space is not None:\n",
    "            # Show most frequent actions with their steering/acceleration values\n",
    "            print(f\"  Top 10 most frequent actions:\")\n",
    "            for action_idx, count in action_counts.most_common(10):\n",
    "                pct = 100 * count / sum(action_counts.values())\n",
    "\n",
    "                # Check if this is a steering-only or 2D action space\n",
    "                if hasattr(adaptive_space, 'get_steering_value'):\n",
    "                    # Steering-only action space\n",
    "                    steering = adaptive_space.get_steering_value(action_idx)\n",
    "                    print(f\"    Action {action_idx:3d}: steering={steering:6.3f} - {count:4d} times ({pct:5.1f}%)\")\n",
    "                elif hasattr(adaptive_space, 'get_action_values'):\n",
    "                    # 2D action space (steering + acceleration)\n",
    "                    steering, accel = adaptive_space.get_action_values(action_idx)\n",
    "                    print(f\"    Action {action_idx:3d}: steering={steering:6.3f}, accel={accel:5.2f} - {count:4d} times ({pct:5.1f}%)\")\n",
    "                else:\n",
    "                    # Fallback\n",
    "                    print(f\"    Action {action_idx:3d}: {count:4d} times ({pct:5.1f}%)\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569362cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results\n",
    "def print_results(results):\n",
    "    \"\"\"Pretty print evaluation results.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Mean Reward:      {results['mean_reward']:8.2f}  {results['std_reward']:.2f}\")\n",
    "    print(f\"Min/Max Reward:   {results['min_reward']:8.2f} / {results['max_reward']:.2f}\")\n",
    "    print(f\"Mean Length:      {results['mean_length']:8.2f}  {results['std_length']:.2f}\")\n",
    "    print(f\"Collision Rate:   {results['collision_rate']:8.1%}\")\n",
    "    print(f\"Success Rate:     {results['success_rate']:8.1%}\")\n",
    "\n",
    "    if \"unique_actions_used\" in results:\n",
    "        print(f\"Unique Actions:   {results['unique_actions_used']:8d}\")\n",
    "\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b9677f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "highway",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
